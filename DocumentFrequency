/*Program to determine the SSN Document Frequency 
 * Penton  Date: 11/22/2013
 * Raghavender Thota 
 * Input: Path to set of documents with personal information including SSN in the form of XXX-XX-XXXX
 * Output: The SSN Number and Its Document Frequency*/
import java.io.IOException;
import java.util.HashSet;
import java.util.Iterator;
import java.util.StringTokenizer;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;

@SuppressWarnings("deprecation")
public class DocumentFrequency {
	
	 //Input: Input line of the document with personal information
	 //Output: <SSN, DocumentName>
	public static class SSNCountMapper extends MapReduceBase implements
			Mapper<LongWritable, Text, Text, Text> {
		@Override
		public void map(LongWritable key, Text value,
				OutputCollector<Text, Text> output, Reporter reporter)
				throws IOException {
			String line = value.toString();
			StringTokenizer tokenizer = new StringTokenizer(line);
			FileSplit fileSplit = null;
			while (tokenizer.hasMoreTokens()) {
				String word = tokenizer.nextToken();
				if (word.matches("^\\d{3}-?\\d{2}-?\\d{4}$")) {
					fileSplit = (FileSplit) reporter.getInputSplit();
					try {
						output.collect(new Text(word), new Text(fileSplit
								.getPath().getName()));
					} catch (Exception e) {
						e.printStackTrace();
					}
				}
			}

		}

	}

	 //Input: <SSN, DocumentName>
	 //Output: <SSN, DocumentFrequency>
	//HastSet is used to ignore the duplicate count of the documents
	public static class SSNCountReducer extends MapReduceBase implements
			Reducer<Text, Text, Text, IntWritable> {
		@Override
		public void reduce(Text key, Iterator<Text> values,
				OutputCollector<Text, IntWritable> output, Reporter reporter)
				throws IOException {
			HashSet<String> uniqueDocuments = new HashSet<String>();
			while (values.hasNext())
				uniqueDocuments.add(values.next().toString());
			try {
				output.collect(new Text("SSN: " + key.toString()
						+ " , number of documents that had this SSN:"),
						new IntWritable(uniqueDocuments.size()));
			} catch (Exception e) {
				e.printStackTrace();
			}
		}

	}

	public static void main(String[] args) throws Exception {
		JobConf job = new JobConf(DocumentFrequency.class);
		job.setJobName("DocumentFrequency");

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.setMapperClass(SSNCountMapper.class);
		job.setReducerClass(SSNCountReducer.class);

		job.setInputFormat(TextInputFormat.class);
		job.setOutputFormat(TextOutputFormat.class);

		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		JobClient.runJob(job);
	}
}
